# ğŸ“ Blog Article Text Analysis

A comprehensive pipeline for extracting, cleaning, and analyzing blog articles for sentiment, readability, and linguistic featuresâ€”all consolidated into a structured Excel report.

---

## ğŸš€ Project Overview

This project automates the process of:
- Extracting blog article titles and content from a list of URLs
- Performing sentiment and readability analysis
- Calculating key linguistic metrics
- Saving all results in a ready-to-use Excel file

---

## âœ¨ Features

- **Automated Web Scraping**: Extracts `<h1>` (title) and `<p>` (content) from each article URL
- **Text Cleaning**: Removes punctuation, lowercases, tokenizes, and removes stopwords
- **Sentiment Analysis**: Computes positive/negative scores, polarity, and subjectivity
- **Readability Metrics**: Calculates FOG index, average sentence length, complex word stats, and more
- **Linguistic Features**: Counts personal pronouns, syllables, word lengths, etc.
- **Excel Output**: Merges all results with original input and exports to Excel

---

## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ Input.xlsx                # Input URLs
â”œâ”€â”€ Src/
â”‚   â”œâ”€â”€ extracted_articles/       # Raw text files per article
â”‚   â”œâ”€â”€ MasterDictionary/         # positive-words.txt, negative-words.txt
â”‚   â”œâ”€â”€ StopWords/                # stopwords.txt
â”‚   â”œâ”€â”€ text-analysis.ipynb       # Main analysis notebook
â”‚   â””â”€â”€ Output Data Structure.xlsx# Final output
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ instruction_document          # Detailed instructions
â””â”€â”€ README.md                     # This file
```

---

## âš¡ï¸ Quickstart

### 1. Clone the Repository
```bash
git clone <repo-url>
cd <project-directory>
```

### 2. Set Up a Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Download NLTK Data (First Run Only)
```python
import nltk
nltk.download('punkt')
```

### 5. Launch Jupyter Notebook
```bash
jupyter lab
# or
jupyter notebook
```
Open `Src/text-analysis.ipynb` and run all cells sequentially.

---

## ğŸ› ï¸ Usage Workflow

1. **Prepare Input**: Add URLs to `Data/Input.xlsx`.
2. **Run Notebook**: Execute all cells in `Src/text-analysis.ipynb`.
3. **Review Output**: Find results in `Src/Output Data Structure.xlsx`.

---

## ğŸ“Š Output

The output Excel file includes:
- `URL_ID`, `URL`
- 13 calculated metrics per article:
  - Positive/Negative Score
  - Polarity/Subjectivity
  - Average Sentence Length
  - Percentage of Complex Words
  - FOG Index
  - Word Count
  - Complex Word Count
  - Syllables Per Word
  - Personal Pronouns
  - Average Word Length

---

## ğŸ“¦ Dependencies

- pandas
- requests
- beautifulsoup4
- nltk
- openpyxl
- textstat

Install all with `pip install -r requirements.txt`.

---

## ğŸ™ Credits

- [NLTK](https://www.nltk.org/)
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)
- [TextStat](https://pypi.org/project/textstat/)

---

## ğŸ“¬ Contact

For questions or suggestions, please open an issue or contact the maintainer.
